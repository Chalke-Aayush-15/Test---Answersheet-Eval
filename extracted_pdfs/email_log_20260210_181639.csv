student,email,subjects,overall_percentage,time,status
Aarya Mayekar Email ID: aaryamayekar971@gmail.com,aaryamayekar971@gmail.com,1,100.0,2026-02-10 17:43:31,Sent
Nitesh Mulam Email ID: niteshmulam30@gmail.com,niteshmulam30@gmail.com,1,100.0,2026-02-10 17:43:35,Sent
Aarya Mayekar Email ID: aaryamayekar971@gmail.com,aaryamayekar971@gmail.com,1,100.0,2026-02-10 17:47:20,Sent
Nitesh Mulam Email ID: niteshmulam30@gmail.com,niteshmulam30@gmail.com,1,75.0,2026-02-10 17:47:24,Sent
"Aarya Mayekar Email ID: aaryamayekar971@gmail.com 1 a: Machine Learning enables computers to learn from data without programming. Main objectives: automatic learning, prediction accuracy improvement, decision making. 1 c: Tradeoff between model simplicity and complexity. Bias error from assumptions, variance from sensitivity to fluctuations. Need medium complexity. 2 a: Tree models splitting data by features. ID3 uses information theory concepts to choose splits that maximize information gain. 2 c: Techniques to reduce overfitting. L1 regularization can zero out features. L2 reduces coefficient magnitudes. Helps generalization. 3 b: Precision: true positives among predicted positives. Recall: true positives among actual positives. F1 combines both metrics. 3 c: Model validation technique. Data divided into subsets. Multiple training/validation rounds. Reduces performance variability estimate. 4 b: First-order optimization algorithm. Moves parameters in negative gradient direction. Learning rate crucial for convergence speed. 4 c: Model performs well on training but poorly on test data. Solutions: simpler models, regularization, more training samples, feature reduction.",aaryamayekar971@gmail.com,1,0.0,2026-02-10 18:16:36,Sent
"Nitesh Mulam Email ID: niteshmulam30@gmail.com 1 a: ML is AI branch creating algorithms that learn from data experience without direct programming. Goals: automatic pattern learning, accurate predictions, decision improvement. 1 b: Supervised learning has labeled training data with known outputs. Examples: spam detection, price prediction. Unsupervised finds patterns in unlabeled data. Examples: customer segmentation, PCA. 2 b: K-means algorithm: Random centroids, assign points, update centroids as mean, repeat until no changes. Uses Euclidean distance. 2 c: Regularization adds penalty to loss function. L1 promotes sparse solutions, L2 shrinks coefficients evenly. Both control model complexity. 3 a: Backpropagation calculates gradients in neural networks. Forward computes output, backward propagates errors to update weights through gradient descent. 3 c: K-fold cross-validation divides data into k parts, trains on k-1, tests on 1, rotates. Gives average performance across folds. 4 a: Support Vector Machines maximize margin between classes. Kernel trick handles non-linear data using kernel functions in high-dimensional space. 4 b: Gradient descent optimization updates parameters against gradient. Learning rate controls step size. Iterates until convergence.",niteshmulam30@gmail.com,1,0.0,2026-02-10 18:16:39,Sent
